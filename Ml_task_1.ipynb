{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "32b0ea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vishal megamart guard prep\n"
     ]
    }
   ],
   "source": [
    "print('vishal megamart guard prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ebce092f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>reason</th>\n",
       "      <th>...</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>romantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>course</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>course</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>home</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex address famsize Pstatus  Medu  Fedu     Mjob      Fjob  reason  \\\n",
       "0     GP   F       U     GT3       A     4   4.0  at_home   teacher  course   \n",
       "1     GP   F       U     GT3       T     1   1.0  at_home     other  course   \n",
       "2     GP   F       U     LE3       T     1   1.0  at_home     other   other   \n",
       "3     GP   F       U     GT3       T     4   2.0   health  services    home   \n",
       "4     GP   F       U     GT3       T     3   3.0    other     other    home   \n",
       "\n",
       "   ... Dalc  health  absences  G1    G2  G3 Feature_1 Feature_2 Feature_3  \\\n",
       "0  ...    1       3       4.0   0  11.0  11      18.0       2.0       1.0   \n",
       "1  ...    1       3       2.0   9  11.0  11      17.0       2.0       1.0   \n",
       "2  ...    2       3       6.0  12  13.0  12      15.0       NaN       3.0   \n",
       "3  ...    1       5       0.0  14  14.0  14      15.0       3.0       1.0   \n",
       "4  ...    1       5       0.0  11  13.0  13      16.0       NaN       2.0   \n",
       "\n",
       "  romantic  \n",
       "0       no  \n",
       "1       no  \n",
       "2       no  \n",
       "3      yes  \n",
       "4       no  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df=pd.read_csv('Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94a1f0",
   "metadata": {},
   "source": [
    "Data cleaning and Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7858bb1",
   "metadata": {},
   "source": [
    "Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d65baf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob',\n",
       "       'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities',\n",
       "       'nursery', 'higher', 'internet', 'romantic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d0eaf84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 3 1 2] 0.32777496318862354\n",
      "[2 3 4 1 0] 0.3493518484148982\n",
      "[3 4 1 0 2] 0.46300707668771063\n",
      "[4 2 3 0 1] 0.5699226580421886\n"
     ]
    }
   ],
   "source": [
    "n = 120\n",
    "mx = 0\n",
    "while (n!=0):\n",
    "    arr = np.random.permutation(5)\n",
    "    codes = {'teacher':arr[0],'health':arr[1],'services':arr[2],'at_home':arr[3],'other':arr[4]}\n",
    "    df['codes'] = df['Mjob'].map(codes)\n",
    "    corr = ( df['Medu'].corr( df['codes']))\n",
    "    if(corr>mx):\n",
    "        mx = corr\n",
    "        print(arr,corr) \n",
    "    n=n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "38a217d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 4 2 1] 0.0838335287831774\n",
      "[2 4 1 3 0] 0.24685619916616763\n",
      "[4 3 2 1 0] 0.3588588106399968\n",
      "[3 4 2 0 1] 0.38872217918214985\n"
     ]
    }
   ],
   "source": [
    "n = 120\n",
    "mx = 0\n",
    "while (n!=0):\n",
    "    arr = np.random.permutation(5)\n",
    "    codes = {'teacher':arr[0],'health':arr[1],'services':arr[2],'at_home':arr[3],'other':arr[4]}\n",
    "    df['codes'] = df['Fjob'].map(codes)\n",
    "    corr = ( df['Fedu'].corr( df['codes']))\n",
    "    if(corr>mx):\n",
    "        mx = corr\n",
    "        print(arr,corr) \n",
    "    n=n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c76ad08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madha\\AppData\\Local\\Temp\\ipykernel_25340\\798503812.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['famsize']=df['famsize'].replace({'GT3': 1, 'LE3': 0})\n",
      "C:\\Users\\madha\\AppData\\Local\\Temp\\ipykernel_25340\\798503812.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Mjob']=df['Mjob'].replace({'teacher': 4, 'health': 3, 'services': 2, 'at_home': 0, 'other': 1})\n",
      "C:\\Users\\madha\\AppData\\Local\\Temp\\ipykernel_25340\\798503812.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Fjob']=df['Fjob'].replace({'teacher': 4, 'health': 3, 'services': 2, 'at_home': 0, 'other': 1})\n",
      "C:\\Users\\madha\\AppData\\Local\\Temp\\ipykernel_25340\\798503812.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['reason']=df['reason'].replace({'home': 0, 'reputation': 1, 'course': 2, 'other': 3})\n",
      "C:\\Users\\madha\\AppData\\Local\\Temp\\ipykernel_25340\\798503812.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['guardian']=df['guardian'].replace({'mother': 1, 'father': 2, 'other': 0})\n"
     ]
    }
   ],
   "source": [
    "df['famsize']=df['famsize'].replace({'GT3': 1, 'LE3': 0})\n",
    "df['Mjob']=df['Mjob'].replace({'teacher': 4, 'health': 3, 'services': 2, 'at_home': 0, 'other': 1})\n",
    "df['Fjob']=df['Fjob'].replace({'teacher': 4, 'health': 3, 'services': 2, 'at_home': 0, 'other': 1})\n",
    "df['reason']=df['reason'].replace({'home': 0, 'reputation': 1, 'course': 2, 'other': 3})\n",
    "df['guardian']=df['guardian'].replace({'mother': 1, 'father': 2, 'other': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aabe7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school          int64\n",
      "sex             int64\n",
      "address         int64\n",
      "famsize       float64\n",
      "Pstatus         int64\n",
      "Medu            int64\n",
      "Fedu          float64\n",
      "Mjob            int64\n",
      "Fjob            int64\n",
      "reason          int64\n",
      "guardian        int64\n",
      "traveltime    float64\n",
      "failures        int64\n",
      "schoolsup       int64\n",
      "famsup          int64\n",
      "paid            int64\n",
      "activities      int64\n",
      "nursery         int64\n",
      "higher          int64\n",
      "internet        int64\n",
      "famrel          int64\n",
      "freetime      float64\n",
      "goout           int64\n",
      "Dalc            int64\n",
      "health          int64\n",
      "absences      float64\n",
      "G1              int64\n",
      "G2            float64\n",
      "G3              int64\n",
      "Feature_1     float64\n",
      "Feature_2     float64\n",
      "Feature_3     float64\n",
      "romantic        int64\n",
      "codes           int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Drop rows with missing values\n",
    "\n",
    "\n",
    "# Step 2: Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Step 3: Loop through object-type columns and encode\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Now all object columns are encoded as integers\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3aa758ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "famsize 50\n",
      "Fedu 73\n",
      "traveltime 73\n",
      "freetime 45\n",
      "absences 69\n",
      "G2 35\n",
      "Feature_1 38\n",
      "Feature_2 46\n",
      "Feature_3 39\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].isna().any():\n",
    "        print(col, df[col].isna().sum())\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ee326a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "06b2738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fedu\n",
       "2.0    0.315972\n",
       "1.0    0.276042\n",
       "4.0    0.203125\n",
       "3.0    0.194444\n",
       "0.0    0.010417\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Fedu'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "598d1f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features by correlation: ['Pstatus', 'Dalc', 'sex', 'Mjob', 'higher']\n",
      "Cross-validation accuracy scores: [0.75409836 0.63333333 0.76666667 0.68333333 0.73333333]\n",
      "Mean accuracy: 0.7141530054644809\n",
      "Mean cross-validation accuracy: 0.7141530054644809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows with NaNs in the target\n",
    "df_clean = df_dropped.dropna(subset=['famsize'])\n",
    "\n",
    "# Step 2: Drop any features (columns) with NaNs\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with the binary target\n",
    "correlations = df_clean.corr().abs()['famsize'].sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Select top 5 features most correlated with 'higher', excluding 'higher' itself\n",
    "top_5_features = correlations.drop('famsize').head(5).index.tolist()\n",
    "print(\"Top 5 features by correlation:\", top_5_features)\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['famsize']  # already mapped to 0/1\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Logistic Regression with cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Step 8: Output results\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "\n",
    "print(\"Mean cross-validation accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d6b81bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features by correlation: ['Medu', 'Fjob', 'Mjob', 'traveltime', 'internet', 'school']\n",
      "Cross-validation accuracy scores: [0.58823529 0.48       0.5        0.38       0.46       0.44      ]\n",
      "Mean accuracy: 0.47470588235294114\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows with NaNs in the target\n",
    "df_clean = df_dropped.dropna(subset=['Fedu'])\n",
    "\n",
    "# Step 2: Drop any features (columns) with NaNs\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with the binary target\n",
    "correlations = df_clean.corr().abs()['Fedu'].sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Select top 5 features most correlated with 'higher', excluding 'higher' itself\n",
    "top_5_features = correlations.drop('Fedu').head(6).index.tolist()\n",
    "print(\"Top 5 features by correlation:\", top_5_features)\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['Fedu']  # already mapped to 0/1\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Logistic Regression with cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=6, scoring='accuracy')\n",
    "\n",
    "# Step 8: Output results\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1078146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features by correlation: ['address', 'Medu', 'Fedu', 'school', 'reason', 'internet', 'Mjob']\n",
      "Cross-validation accuracy scores: [0.58139535 0.60465116 0.62790698 0.62790698 0.72093023 0.58139535\n",
      " 0.46511628]\n",
      "Mean accuracy: 0.6013289036544851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows with NaNs in the target\n",
    "df_clean = df_dropped.dropna(subset=['traveltime'])\n",
    "\n",
    "# Step 2: Drop any features (columns) with NaNs\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with the binary target\n",
    "correlations = df_clean.corr().abs()['traveltime'].sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Select top 5 features most correlated with 'higher', excluding 'higher' itself\n",
    "top_5_features = correlations.drop('traveltime').head(7).index.tolist()\n",
    "print(\"Top 5 features by correlation:\", top_5_features)\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['traveltime']  # already mapped to 0/1\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Logistic Regression with cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=7, scoring='accuracy')\n",
    "\n",
    "# Step 8: Output results\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ecdc22e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features by correlation: ['G1', 'G2', 'absences', 'famrel', 'G3']\n",
      "Cross-validation accuracy scores: [0.75409836 0.78333333 0.8        0.76666667 0.7       ]\n",
      "Mean accuracy: 0.7608196721311475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows with NaNs in the target\n",
    "df_clean = df_dropped.dropna(subset=['higher'])\n",
    "\n",
    "# Step 2: Drop any features (columns) with NaNs\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with the binary target\n",
    "correlations = df_clean.corr().abs()['higher'].sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Select top 5 features most correlated with 'higher', excluding 'higher' itself\n",
    "top_5_features = correlations.drop('higher').head(5).index.tolist()\n",
    "print(\"Top 5 features by correlation:\", top_5_features)\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['higher']  # already mapped to 0/1\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Logistic Regression with cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Step 8: Output results\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8adf2e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features by correlation: ['goout', 'sex', 'famrel', 'paid', 'Feature_3']\n",
      "Cross-validation accuracy scores: [0.36065574 0.43333333 0.45       0.3        0.43333333]\n",
      "Mean accuracy: 0.39546448087431696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows with NaNs in the target\n",
    "df_clean = df_dropped.dropna(subset=['freetime'])\n",
    "\n",
    "# Step 2: Drop any features (columns) with NaNs\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with the binary target\n",
    "correlations = df_clean.corr().abs()['freetime'].sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Select top 5 features most correlated with 'higher', excluding 'higher' itself\n",
    "top_5_features = correlations.drop('freetime').head(5).index.tolist()\n",
    "print(\"Top 5 features by correlation:\", top_5_features)\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['freetime']  # already mapped to 0/1\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Logistic Regression with cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Step 8: Output results\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b9f8eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation R² scores: [-0.44721121 -0.0528337  -0.01852673 -0.39573545 -2.87891604]\n",
      "Mean R² score: -0.7586446276771873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "correlations = df_dropped.corr().abs()\n",
    "\n",
    "# Get top 5 features most correlated with the target (excluding target itself)\n",
    "top_5_features = correlations['absences'].drop('absences').sort_values(ascending=False).head(5).index.tolist()\n",
    "\n",
    "# Define features and numerical target\n",
    "X = df_dropped[top_5_features]\n",
    "y = df_dropped['absences']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Regression model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Cross-validation with scoring='r2' (or 'neg_mean_squared_error')\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"Cross-validation R² scores:\", scores)\n",
    "print(\"Mean R² score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "affed1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features: ['G3', 'G1', 'failures', 'Medu', 'school']\n",
      "Cross-validation R² scores: [0.81953223 0.8321389  0.90790962 0.87036853 0.84560566]\n",
      "Mean R² score: 0.8551109878766395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows where the target (G2) is NaN\n",
    "df_clean = df.dropna(subset=['G2'])\n",
    "\n",
    "# Step 2: Drop columns (features) that contain any NaN values\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with target 'G2'\n",
    "correlations = df_clean.corr().abs()\n",
    "\n",
    "# Step 4: Get top 5 features most correlated with 'G2', excluding 'G2' itself\n",
    "top_5_features = (\n",
    "    correlations['G2']\n",
    "    .drop('G2')\n",
    "    .sort_values(ascending=False)\n",
    "    .head(5)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['G2']\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Linear Regression with cross-validation\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')\n",
    "\n",
    "# Output results\n",
    "print(\"Top 5 features:\", top_5_features)\n",
    "print(\"Cross-validation R² scores:\", scores)\n",
    "print(\"Mean R² score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "65284545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature_2\n",
       "2.0    0.462687\n",
       "1.0    0.338308\n",
       "3.0    0.145937\n",
       "4.0    0.053068\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Feature_1'].value_counts()\n",
    "df['Feature_2'].value_counts()  \n",
    "df['Feature_2'].value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "824c0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features by correlation: ['Dalc', 'goout', 'sex', 'Feature_2', 'codes']\n",
      "Cross-validation accuracy scores: [0.45901639 0.45       0.48333333 0.51666667 0.48333333]\n",
      "Mean accuracy: 0.47846994535519133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows with NaNs in the target\n",
    "df_clean = df_dropped.dropna(subset=['Feature_3'])\n",
    "\n",
    "# Step 2: Drop any features (columns) with NaNs\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with the binary target\n",
    "correlations = df_clean.corr().abs()['Feature_3'].sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Select top 5 features most correlated with 'higher', excluding 'higher' itself\n",
    "top_5_features = correlations.drop('Feature_3').head(5).index.tolist()\n",
    "print(\"Top 5 features by correlation:\", top_5_features)\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['Feature_3']  # already mapped to 0/1\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Logistic Regression with cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Step 8: Output results\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "550c4d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features by correlation: ['G1', 'G2', 'Feature_3', 'G3', 'sex']\n",
      "Cross-validation accuracy scores: [0.49180328 0.41666667 0.51666667 0.43333333 0.55      ]\n",
      "Mean accuracy: 0.4816939890710382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Drop rows with NaNs in the target\n",
    "df_clean = df_dropped.dropna(subset=['Feature_2'])\n",
    "\n",
    "# Step 2: Drop any features (columns) with NaNs\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "\n",
    "# Step 3: Compute absolute correlations with the binary target\n",
    "correlations = df_clean.corr().abs()['Feature_2'].sort_values(ascending=False)\n",
    "\n",
    "# Step 4: Select top 5 features most correlated with 'higher', excluding 'higher' itself\n",
    "top_5_features = correlations.drop('Feature_2').head(5).index.tolist()\n",
    "print(\"Top 5 features by correlation:\", top_5_features)\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df_clean[top_5_features]\n",
    "y = df_clean['Feature_2']  # already mapped to 0/1\n",
    "\n",
    "# Step 6: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 7: Train Logistic Regression with cross-validation\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Step 8: Output results\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a549d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7846153846153846\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.78      1.00      0.88       102\n",
      "           2       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.78       130\n",
      "   macro avg       0.26      0.33      0.29       130\n",
      "weighted avg       0.62      0.78      0.69       130\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madha\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\madha\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\madha\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# === Step 1: Copy dataframe to avoid modifying original ===\n",
    "df_model = df.copy()\n",
    "\n",
    "# === Step 2: Impute Feature_1 with value distribution ===\n",
    "value_probs = {\n",
    "    17.0: 0.278232,\n",
    "    16.0: 0.276596,\n",
    "    18.0: 0.212766,\n",
    "    15.0: 0.168576,\n",
    "    19.0: 0.049100,\n",
    "    20.0: 0.009820,\n",
    "    21.0: 0.003273,\n",
    "    22.0: 0.001637\n",
    "}\n",
    "values = list(value_probs.keys())\n",
    "probs = list(value_probs.values())\n",
    "\n",
    "# Add missing indicator column\n",
    "df_model['Feature_1_missing'] = df_model['Feature_1'].isna().astype(int)\n",
    "\n",
    "# Impute missing values\n",
    "num_missing = df_model['Feature_1'].isna().sum()\n",
    "df_model.loc[df_model['Feature_1'].isna(), 'Feature_1'] = np.random.choice(values, size=num_missing, p=probs)\n",
    "\n",
    "# === Step 3: Optional — Bin the Feature_1 (can improve linear model performance) ===\n",
    "df_model['Feature_1_binned'] = pd.cut(df_model['Feature_1'], bins=[14, 16, 18, 20, 22], labels=False)\n",
    "\n",
    "# === Step 4: Prepare features and target ===\n",
    "# You can use the binned version, original version, and missing indicator\n",
    "feature_columns = ['Feature_1_binned', 'Feature_1_missing']  # add more good features here\n",
    "target_column = 'higher'  # assuming already 0/1\n",
    "\n",
    "X = df_model[feature_columns]\n",
    "y = df_model[target_column]\n",
    "\n",
    "# === Step 5: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# === Step 6: Scale features ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 7: Train logistic regression ===\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Step 8: Evaluate ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446b597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Imputing 'famsize' using top 5 features: ['Pstatus', 'nursery', 'sex', 'Fjob', 'address']\n",
      "✅ No missing values in 'famsize'\n",
      "\n",
      "▶ Imputing 'traveltime' using top 5 features: ['address', 'Medu', 'school', 'Mjob', 'G1']\n",
      "✅ No missing values in 'traveltime'\n",
      "\n",
      "▶ Imputing 'Fedu' using top 5 features: ['Medu', 'Fjob', 'Mjob', 'G2', 'G3']\n",
      "✅ Imputed 73 missing values in 'Fedu'\n",
      "\n",
      "▶ Imputing 'Feature_3' using top 5 features: ['Dalc', 'goout', 'sex', 'G3', 'G2']\n",
      "✅ Imputed 39 missing values in 'Feature_3'\n",
      "\n",
      "▶ Imputing 'G2' using top 5 features: ['G3', 'G1', 'failures', 'school', 'Medu']\n",
      "✅ No missing values in 'G2'\n",
      "\n",
      "🔍 Missing values after imputation:\n",
      "famsize       0\n",
      "traveltime    0\n",
      "G2            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Helper function: impute a column using predictive model\n",
    "def predictive_impute(df, target_col, model_type='logistic', round_output=False):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Get top 5 most correlated features (no NaNs) with target_col\n",
    "    correlations = df_copy.corr(numeric_only=True)[target_col].drop(target_col).abs()\n",
    "    non_nan_features = df_copy.columns[df_copy.notna().all()].tolist()\n",
    "    top_5_features = correlations[correlations.index.isin(non_nan_features)].sort_values(ascending=False).head(5).index.tolist()\n",
    "\n",
    "    print(f\"\\n▶ Imputing '{target_col}' using top 5 features: {top_5_features}\")\n",
    "    \n",
    "    # Split into known and unknown\n",
    "    known = df_copy[df_copy[target_col].notna()]\n",
    "    unknown = df_copy[df_copy[target_col].isna()]\n",
    "    \n",
    "    if unknown.empty:\n",
    "        print(f\"✅ No missing values in '{target_col}'\")\n",
    "        return df_copy\n",
    "    \n",
    "    X_known = known[top_5_features]\n",
    "    y_known = known[target_col]\n",
    "    \n",
    "    X_unknown = unknown[top_5_features]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_known_scaled = scaler.fit_transform(X_known)\n",
    "    X_unknown_scaled = scaler.transform(X_unknown)\n",
    "    \n",
    "    # Choose model type\n",
    "    if model_type == 'logistic':\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "    elif model_type == 'linear':\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'logistic' or 'linear'\")\n",
    "    \n",
    "    model.fit(X_known_scaled, y_known)\n",
    "    predictions = model.predict(X_unknown_scaled)\n",
    "    \n",
    "    if round_output:\n",
    "        predictions = np.round(predictions).astype(int)\n",
    "    \n",
    "    # Impute back into original DataFrame\n",
    "    df.loc[df[target_col].isna(), target_col] = predictions\n",
    "    print(f\"✅ Imputed {len(predictions)} missing values in '{target_col}'\")\n",
    "    return df\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "# Start with your actual DataFrame\n",
    "# Make sure columns are already encoded\n",
    "# Assumes df is defined and contains the mentioned columns\n",
    "df = predictive_impute(df, 'famsize', model_type='logistic')\n",
    "df = predictive_impute(df, 'traveltime', model_type='logistic')\n",
    "df= predictive_impute(df, 'Fedu', model_type='logistic')\n",
    "df = predictive_impute(df, 'Feature_3', model_type='logistic')\n",
    "df = predictive_impute(df, 'G2', model_type='linear', round_output=True)\n",
    "\n",
    "# Final check\n",
    "print(\"\\n🔍 Missing values after imputation:\")\n",
    "print(df[['famsize', 'traveltime', 'G2']].isna().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9e2d9de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imputed 0 missing values in 'Feature_2' with mode: 2.0\n",
      "✅ Imputed 0 missing values in 'Feature_1' with mode: 17.0\n",
      "✅ Imputed 0 missing values in 'absences' with mode: 0.0\n",
      "✅ Imputed 45 missing values in 'freetime' with mode: 3.0\n",
      "\n",
      "🔍 Missing values remaining:\n",
      "Feature_2    0\n",
      "Feature_1    0\n",
      "absences     0\n",
      "freetime     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madha\\AppData\\Local\\Temp\\ipykernel_25340\\3020811919.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(mode_val, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Function to impute a column with its mode\n",
    "def mode_impute(df, column):\n",
    "    mode_val = df[column].mode()[0]\n",
    "    num_missing = df[column].isna().sum()\n",
    "    df[column].fillna(mode_val, inplace=True)\n",
    "    print(f\"✅ Imputed {num_missing} missing values in '{column}' with mode: {mode_val}\")\n",
    "\n",
    "# Apply mode imputation to the specified columns\n",
    "for col in ['Feature_2', 'Feature_1', 'absences','freetime']:\n",
    "    mode_impute(df, col)\n",
    "\n",
    "# Final check\n",
    "print(\"\\n🔍 Missing values remaining:\")\n",
    "print(df[['Feature_2', 'Feature_1', 'absences','freetime']].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "79550dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: []\n"
     ]
    }
   ],
   "source": [
    "cols_with_na = df.columns[df.isna().any()].tolist()\n",
    "print(\"Columns with missing values:\", cols_with_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88179e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
